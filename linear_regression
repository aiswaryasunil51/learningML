
Linear Regression is a fundamental algorithm in ML used to predict a continuous value. It tries to find the relationship between two variables:
1) independent variable(x): input or cause
2) Dependent Variable(y): output or effect

Our goal is to draw a straight line that passes as close as possible to all the data points. This line is y = mx + c.

This algorithm tries to find the specific values m and c that minimize the error, which is calculated using mean squared error (MSE).

In the code below, we try to predict a studentâ€™s exam score based on their study hours.
Sample data used is a list of students, how long they studied (x), and what they scored(y).
The computer guesses a flat line (y = 0). It assumes studying makes no difference.
It checks how wrong it is, that is, the error and slightly tilts the line upward to reduce that error, and repeats this 1000 times.




Code in Python:
import pandas as pd
import matplotlib.pyplot as plt




data = pd.read_csv('data.csv')


plt.scatter(data.studytime, data.score)
#plt.show()




def loss_function(m, b, points):
    total_error = 0
    for i in range(len(points)):
        x = points.iloc[i].studytime
        y = points.iloc[i].score
        total_error += (y - (m * x + b)) ** 2
    total_error / float(len(points))


def gradient_descent(m_now, b_now, points, L):
    m_gradient = 0
    b_gradient = 0


    n = (len(points))


    for i in range(n):
        x = points.iloc[i].studytime
        y = points.iloc[i].score


        m_gradient += -(2/n) * x * (y - (m_now * x + b_now))
        b_gradient += -(2/n) * (y - (m_now * x + b_now))


    m = m_now - L * m_gradient
    b = b_now - L * b_gradient
    return m, b


m=0
b=0
L = 0.0001
epochs = 1000


for i in range(epochs):
    if i % 50 == 0:
        print(f"Epoch: {i}")
    m, b = gradient_descent(m, b, data, L)


print(m, b)


plt.scatter(data.studytime, data.score, color = "black")
plt.plot(list(range(1, 10)), [m*x + b for x in range(1, 10)], color = "red")
plt.show()










